from fastapi import FastAPI, HTTPException, Body
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
import rag_engine
import os
from dotenv import load_dotenv
from pinecone import Pinecone
from google import genai

# 1. Setup Environment & AI Client
load_dotenv()

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Key ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°
gemini_key = os.environ.get("GEMINI_API_KEY")
pinecone_key = os.environ.get("PINECONE_API_KEY")

if not gemini_key or not pinecone_key:
    raise ValueError("‚ùå Error: ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà GEMINI_API_KEY ‡πÅ‡∏•‡∏∞ PINECONE_API_KEY ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå .env ‡∏´‡∏£‡∏∑‡∏≠ Cloud Run Variables")

# Init Clients
client = genai.Client(api_key=gemini_key)
pc = Pinecone(api_key=pinecone_key)
index = pc.Index(rag_engine.PINECONE_INDEX_NAME)

# 2. Setup FastAPI App
app = FastAPI(title="AI Developer Assistant API")

# 3. Setup CORS (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Vercel ‡πÅ‡∏•‡∏∞ Localhost)
app.add_middleware(
    CORSMiddleware,
    # ‡πÉ‡∏™‡πà * ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Vercel ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏î‡πâ‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô (‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏∞‡∏£‡∏∞‡∏ö‡∏∏‡πÇ‡∏î‡πÄ‡∏°‡∏ô‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡∏Å‡πá‡πÑ‡∏î‡πâ)
    allow_origins=["*"], 
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# 4. Define Data Models
class StoryRequest(BaseModel):
    story_text: str

class RepoRequest(BaseModel):
    repo_url: str

class ChatRequest(BaseModel):
    question: str

# 5. API Endpoints

@app.get("/")
def read_root():
    return {"status": "ok", "message": "üöÄ AI Assistant Backend is running (Optimized RAG)!"}

@app.post("/analyze-story")
def analyze_story(request: StoryRequest):
    """
    ‡∏£‡∏±‡∏ö Jira Story Text -> ‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ AI -> ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Markdown
    """
    try:
        system_prompt = """
        You are a Senior Software Architect.
        Analyze the following Jira Story and break it down into technical sub-tasks.
        Format the output as Markdown.
        
        Provide the response in this structure:
        1.  **Frontend Tasks** (Angular/React)
        2.  **Backend Tasks** (.NET/Node.js)
        3.  **Database Changes**
        4.  **Test Cases**
        
        Here is the story:
        """
        
        response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=system_prompt + request.story_text
        )
        
        return {
            "success": True,
            "markdown_result": response.text
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/ingest")
async def ingest_repository(request: RepoRequest):
    """
    ‡∏£‡∏±‡∏ö GitHub URL -> ‡∏î‡∏π‡∏î Code -> ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏°‡∏≠‡∏á (Optimized Version)
    ‡πÉ‡∏ä‡πâ rag_engine ‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏ó‡∏≥ Batching + Smart Splitting
    """
    try:
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Engine ‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏°‡πà (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á client ‡πÑ‡∏õ ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ engine init ‡πÄ‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß)
        result = rag_engine.ingest_repo(request.repo_url)
        return result
    except Exception as e:
        print(f"Ingest Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/ask-codebase")
async def ask_codebase(request: ChatRequest):
    """
    RAG Search + Persona Generation
    """
    try:
        user_query = request.question.strip()

        # --- STEP 1: Search Logic (‡∏ó‡∏≥‡πÉ‡∏ô main ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏∏‡∏° Logic ‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢) ---
        
        # 1. ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô Vector
        question_embedding = client.models.embed_content(
            model="text-embedding-004",
            contents=user_query
        )

        # 2. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô Pinecone (‡πÉ‡∏ä‡πâ top_k=5 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î ‡πÑ‡∏°‡πà‡πÄ‡∏¢‡∏≠‡∏∞‡∏à‡∏ô AI ‡∏°‡∏∂‡∏ô)
        search_results = index.query(
            vector=question_embedding.embeddings[0].values,
            top_k=5, 
            include_metadata=True
        )

        # 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á Context String
        context_text = ""
        found_sources = []
        for match in search_results.matches:
            if match.score > 0.45: # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ç‡∏¢‡∏∞‡∏ó‡∏¥‡πâ‡∏á (‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ 45% ‡πÑ‡∏°‡πà‡πÄ‡∏≠‡∏≤)
                context_text += f"\n--- File: {match.metadata.get('source', 'unknown')} ---\n{match.metadata.get('text', '')}\n"
                found_sources.append(match.metadata.get('source', 'unknown'))

        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ Context ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏û‡∏≠
        if not context_text:
            context_text = "No specific code found in the repository matching this question."

        # --- STEP 2: Persona Logic (‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì) ---
        
        role_prompt = "You are a Senior Developer. Answer the question based on the provided code context."
        
        if user_query.lower().startswith("/refactor"):
            role_prompt = "You are a Clean Code Expert. Refactor the code for better readability, performance, and maintainability."
        elif user_query.lower().startswith("/test"):
            role_prompt = "You are a QA Automation Engineer. Write comprehensive unit tests (using Pytest or Jest) for the code."
        elif user_query.lower().startswith("/security"):
            role_prompt = "You are a Security Auditor. Analyze the code for vulnerabilities (OWASP Top 10) and suggest fixes."
        elif user_query.lower().startswith("/explain"):
             role_prompt = "You are a Technical Instructor. Explain the logic step-by-step in simple terms."
        elif user_query.lower().startswith("/diagram"):
             role_prompt = "You are a System Architect. Create a Mermaid.js diagram (`flowchart TD` or `sequenceDiagram`) to visualize the flow."

        # --- STEP 3: Final Prompt Construction ---
        
        prompt = f"""
        {role_prompt}
        
        INSTRUCTIONS:
        1. Base your answer PRIMARILY on the "Code Context" provided below.
        2. If the context doesn't contain the answer, state that clearly. Do not make up code.
        3. Use Markdown formatting for code blocks.
        4. Be concise and to the point.
        
        User Question: {user_query}
        
        Code Context (Retrieved from Repo):
        {context_text}
        
        Answer:
        """
        
        # --- STEP 4: Generate Answer ---
        response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=prompt
        )
        
        return {
            "answer": response.text,
            "sources": list(set(found_sources)) # ‡∏™‡πà‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡πÑ‡∏õ‡πÉ‡∏´‡πâ Frontend ‡∏î‡∏π‡∏î‡πâ‡∏ß‡∏¢
        }

    except Exception as e:
        print(f"Chat Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))