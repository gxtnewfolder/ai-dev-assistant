import os
import shutil
import git
import chromadb
import stat
from google import genai
import json

# Config
DB_PATH = "./chroma_db"
REPO_PATH = "./temp_repo"

# Init ChromaDB
chroma_client = chromadb.PersistentClient(path=DB_PATH)
collection = chroma_client.get_or_create_collection(name="codebase")

def remove_readonly(func, path, _):
    """‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏ü‡∏•‡πå Read-only ‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÑ‡∏î‡πâ ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡∏±‡πà‡∏á‡∏•‡∏ö"""
    os.chmod(path, stat.S_IWRITE)
    func(path)

def get_embedding(text, client):
    """‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡πÄ‡∏õ‡πá‡∏ô list ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (Vector) ‡∏î‡πâ‡∏ß‡∏¢ Gemini"""
    result = client.models.embed_content(
        model="text-embedding-004",
        contents=text
    )
    return result.embeddings[0].values

def ingest_repo(repo_url: str, client: genai.Client):
    """
    1. Clone Repo
    2. ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå
    3. ‡∏™‡∏£‡πâ‡∏≤‡∏á Vector
    4. ‡πÄ‡∏Å‡πá‡∏ö‡∏•‡∏á ChromaDB
    """
    if os.path.exists(REPO_PATH):
        print("üóëÔ∏è Removing old repo...")
        shutil.rmtree(REPO_PATH, onerror=remove_readonly)
    
    print(f"üì• Cloning {repo_url}...")
    git.Repo.clone_from(repo_url, REPO_PATH)

    documents = []
    metadatas = []
    ids = []
    
    print("üìÇ Processing files...")
    allowed_ext = {'.py', '.js', '.ts', '.tsx', '.jsx', '.cs', '.java', '.html', '.css', '.md', '.json', '.go', '.rs'}
    
    for root, dirs, files in os.walk(REPO_PATH):
        if 'node_modules' in root or '.git' in root or '__pycache__' in root or 'dist' in root or 'build' in root:
            continue
            
        for file in files:
            ext = os.path.splitext(file)[1]
            if ext in allowed_ext:
                file_path = os.path.join(root, file)
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        doc_id = file_path.replace(REPO_PATH, "")
                        
                        # Chunking: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô 2000 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ ‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏î‡πÅ‡∏ö‡πà‡∏á (Simple Chunking)
                        chunk_size = 2000
                        for i in range(0, len(content), chunk_size):
                            chunk = content[i:i+chunk_size]
                            chunk_id = f"{doc_id}_part{i//chunk_size}"
                            
                            # ‡πÄ‡∏û‡∏¥‡πà‡∏° Context ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏õ‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏î‡πâ‡∏ß‡∏¢ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Vector ‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô
                            enriched_content = f"File: {doc_id}\nCode:\n{chunk}"

                            documents.append(enriched_content)
                            metadatas.append({"path": doc_id, "language": ext})
                            ids.append(chunk_id)

                except Exception as e:
                    print(f"Skipping {file}: {e}")

    if not documents:
        return {"status": "warning", "message": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå Code ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÉ‡∏ô Repo ‡∏ô‡∏µ‡πâ"}

    print(f"üß† Embedding {len(documents)} chunks... (‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤)")
    
    existing_ids = collection.get()['ids']
    if existing_ids:
        collection.delete(ids=existing_ids)

    # Batch Process (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß)
    batch_size = 50
    for i in range(0, len(documents), batch_size):
        batch_docs = documents[i:i+batch_size]
        batch_ids = ids[i:i+batch_size]
        batch_meta = metadatas[i:i+batch_size]
        
        try:
            # ‡πÉ‡∏ä‡πâ loop embed ‡∏ó‡∏µ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏±‡∏ô error limit (‡∏ñ‡πâ‡∏≤ production ‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ batch embed)
            batch_embeddings = [get_embedding(doc, client) for doc in batch_docs]
            
            collection.add(
                ids=batch_ids,
                embeddings=batch_embeddings,
                metadatas=batch_meta,
                documents=batch_docs
            )
            print(f"   ‚úÖ Indexed batch {i} - {i+len(batch_docs)}")
        except Exception as e:
            print(f"   ‚ùå Failed batch {i}: {e}")

    return {"status": "success", "files_processed": len(documents)}

# üî• FEATURE ‡πÉ‡∏´‡∏°‡πà: Query Expansion
def expand_query(original_query: str, client: genai.Client):
    """‡πÉ‡∏ä‡πâ AI ‡∏Ñ‡∏¥‡∏î Keyword ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö Code"""
    prompt = f"""
    You are an expert software engineer.
    The user is searching for code in a repository.
    Generate 3-5 technical keywords or related terms that might appear in the codebase for this query.
    
    User Query: "{original_query}"
    
    Output ONLY a JSON list of strings. Example: ["auth", "login_controller", "jwt_token"]
    """
    try:
        response = client.models.generate_content(
            model="gemini-1.5-flash",
            contents=prompt,
            config={'response_mime_type': 'application/json'}
        )
        keywords = json.loads(response.text)
        return keywords
    except:
        return [original_query] # ‡∏ñ‡πâ‡∏≤ error ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡πÄ‡∏î‡∏¥‡∏°

def query_codebase(query: str, client: genai.Client, n_results=5):
    """Smart Search: ‡∏´‡∏≤‡∏î‡πâ‡∏ß‡∏¢ Query ‡πÄ‡∏î‡∏¥‡∏° + Expanded Keywords"""
    
    # 1. Expand Query
    print(f"üîé Expanding query: {query}")
    keywords = expand_query(query, client)
    search_terms = [query] + keywords
    print(f"   Keywords: {search_terms}")

    # 2. Search ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≥ (‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå)
    all_results = {} # ‡πÉ‡∏ä‡πâ Dict ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏î‡∏ï‡∏±‡∏ß‡∏ã‡πâ‡∏≥ (Deduplicate)
    
    for term in search_terms:
        term_vector = get_embedding(term, client)
        results = collection.query(
            query_embeddings=[term_vector],
            n_results=2 # ‡πÄ‡∏≠‡∏≤‡∏Ñ‡∏≥‡∏•‡∏∞ 2 ‡πÑ‡∏ü‡∏•‡πå‡∏û‡∏≠ ‡πÄ‡∏î‡∏µ‡πã‡∏¢‡∏ß‡πÄ‡∏¢‡∏≠‡∏∞‡πÄ‡∏Å‡∏¥‡∏ô
        )
        
        if results['documents']:
            for i, doc in enumerate(results['documents'][0]):
                doc_id = results['ids'][0][i]
                if doc_id not in all_results:
                    all_results[doc_id] = {
                        "content": doc,
                        "metadata": results['metadatas'][0][i],
                        "score": results['distances'][0][i] if results['distances'] else 0
                    }

    # 3. Sort by relevance (Distance ‡∏ô‡πâ‡∏≠‡∏¢ = ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏°‡∏≤‡∏Å)
    sorted_docs = sorted(all_results.values(), key=lambda x: x['score'])
    
    # 4. Format Output (‡πÄ‡∏≠‡∏≤ top 5 ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å Keyword ‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ô)
    final_context = []
    for item in sorted_docs[:n_results]:
        meta = item['metadata']
        final_context.append(f"--- File: {meta['path']} ---\n{item['content']}\n")
            
    return "\n".join(final_context)