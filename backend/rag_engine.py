import os
import shutil
import git
import time
from dotenv import load_dotenv
from pinecone import Pinecone, ServerlessSpec
from google import genai
from langchain.text_splitter import RecursiveCharacterTextSplitter, Language
from langchain_core.documents import Document

# 1. ‡πÇ‡∏´‡∏•‡∏î Environment Variables
load_dotenv()

# 2. Config ‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≤‡∏á‡πÜ
REPO_PATH = "./temp_repo"
EMBEDDING_MODEL = "text-embedding-004"
PINECONE_INDEX_NAME = "codebase"
BATCH_SIZE = 100  # ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡πâ‡∏≤ Pinecone ‡∏ó‡∏µ‡∏•‡∏∞ 100 ‡∏Å‡πâ‡∏≠‡∏ô (‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤‡∏™‡πà‡∏á‡∏ó‡∏µ‡∏•‡∏∞‡∏≠‡∏±‡∏ô)

# 3. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô Pinecone
api_key = os.environ.get("PINECONE_API_KEY")
if not api_key:
    raise ValueError("‚ùå PINECONE_API_KEY not found in .env")

pc = Pinecone(api_key=api_key)
client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))

# ‡∏™‡∏£‡πâ‡∏≤‡∏á Index ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
if PINECONE_INDEX_NAME not in pc.list_indexes().names():
    pc.create_index(
        name=PINECONE_INDEX_NAME,
        dimension=768,
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1")
    )

index = pc.Index(PINECONE_INDEX_NAME)

# --- Helper Function: Batch Generator ---
def chunks(iterable, batch_size=100):
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡πâ‡∏≠‡∏ô‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÅ‡∏ö‡∏ö Batch"""
    it = iter(iterable)
    chunk = list(it)
    while chunk:
        # ‡∏ñ‡πâ‡∏≤ chunk ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏ï‡πá‡∏° batch_size ‡πÉ‡∏´‡πâ‡πÄ‡∏ï‡∏¥‡∏°
        while len(chunk) < batch_size:
             try:
                 chunk.append(next(it))
             except StopIteration:
                 break
        yield chunk[:batch_size]
        chunk = chunk[batch_size:]
        # ‡∏ñ‡πâ‡∏≤ chunk ‡∏´‡∏°‡∏î‡πÅ‡∏•‡πâ‡∏ß ‡πÅ‡∏ï‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ (‡∏Å‡∏£‡∏ì‡∏µ‡∏´‡∏•‡∏∏‡∏î loop while ‡πÉ‡∏ô)
        if not chunk and len(chunk) < batch_size:
             try:
                # ‡∏•‡∏≠‡∏á‡∏î‡∏∂‡∏á‡∏ï‡∏±‡∏ß‡∏ñ‡∏±‡∏î‡πÑ‡∏õ ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡πá‡πÄ‡∏£‡∏¥‡πà‡∏° loop ‡πÉ‡∏´‡∏°‡πà
                 item = next(it) 
                 chunk.append(item) 
             except StopIteration:
                 break
                 
# ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢‡∏Å‡∏ß‡πà‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö list slicing
def batch_iterate(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

def ingest_repo(repo_url: str):
    """‡πÇ‡∏´‡∏•‡∏î Repo ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Vector ‡∏•‡∏á Pinecone"""
    print(f"üöÄ Starting ingestion for: {repo_url}")
    
    # 1. Clear ‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πà‡∏≤
    if os.path.exists(REPO_PATH):
        shutil.rmtree(REPO_PATH)

    # 2. Clone ‡πÅ‡∏ö‡∏ö depth=1 (‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡∏Å ‡πÑ‡∏°‡πà‡πÄ‡∏≠‡∏≤‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÄ‡∏Å‡πà‡∏≤)
    print("üì• Cloning repository (Depth=1)...")
    git.Repo.clone_from(repo_url, REPO_PATH, depth=1)

    documents = []
    
    # 3. ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Splitter ‡πÉ‡∏´‡πâ‡∏â‡∏•‡∏≤‡∏î
    print("üìÇ Processing files...")
    for root, dirs, files in os.walk(REPO_PATH):
        # ‡∏Ç‡πâ‡∏≤‡∏°‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
        if '.git' in dirs: dirs.remove('.git')
        if 'node_modules' in dirs: dirs.remove('node_modules')
        
        for file in files:
            file_path = os.path.join(root, file)
            # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå Code ‡∏´‡∏•‡∏±‡∏Å‡πÜ
            if file.endswith(('.py', '.js', '.jsx', '.ts', '.tsx', '.md', '.txt', '.html', '.css', '.cs')):
                try:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                        
                    relative_path = os.path.relpath(file_path, REPO_PATH)
                    
                    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Splitter ‡∏ï‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤
                    if file.endswith('.py'):
                        splitter = RecursiveCharacterTextSplitter.from_language(
                            language=Language.PYTHON, chunk_size=1000, chunk_overlap=200
                        )
                    elif file.endswith(('.js', '.jsx', '.ts', '.tsx')):
                        splitter = RecursiveCharacterTextSplitter.from_language(
                            language=Language.JS, chunk_size=1000, chunk_overlap=200
                        )
                    elif file.endswith('.md'):
                        splitter = RecursiveCharacterTextSplitter.from_language(
                            language=Language.MARKDOWN, chunk_size=1000, chunk_overlap=200
                        )
                    elif file.endswith('.cs'):
                        splitter = RecursiveCharacterTextSplitter.from_language(
                            language=Language.CSHARP, chunk_size=1000, chunk_overlap=200
                        )
                    else:
                        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

                    # ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥
                    chunks_data = splitter.create_documents([content])
                    
                    for i, chunk in enumerate(chunks_data):
                        documents.append({
                            "id": f"{relative_path}_{i}",
                            "text": chunk.page_content,
                            "source": relative_path
                        })
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è Skipping {file}: {e}")

    # 4. ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Vector (Embedding) ‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ Pinecone ‡πÅ‡∏ö‡∏ö Batch
    print(f"üß† Embedding {len(documents)} chunks...")
    
    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Upsert
    vectors_to_upsert = []
    
    # ‡πÉ‡∏ä‡πâ Batch ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å API ‡∏Ç‡∏≠‡∏á Gemini ‡πÅ‡∏•‡∏∞ Pinecone
    for i, batch_docs in enumerate(batch_iterate(documents, BATCH_SIZE)):
        print(f"   Processing batch {i+1}...")
        
        # ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Text ‡πÑ‡∏õ‡∏ó‡∏≥ Embedding
        texts = [doc['text'] for doc in batch_docs]
        
        try:
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Gemini ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏≤‡∏¢ Embedding (‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤)
            embeddings = client.models.embed_content(
                model=EMBEDDING_MODEL,
                contents=texts,
            )
            
            # ‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà Vector ‡∏Å‡∏±‡∏ö Metadata
            for doc, embedding in zip(batch_docs, embeddings.embeddings):
                vectors_to_upsert.append({
                    "id": doc['id'],
                    "values": embedding.values,
                    "metadata": {"text": doc['text'], "source": doc['source']}
                })
        except Exception as e:
            print(f"‚ùå Error embedding batch: {e}")

    # 5. Upsert ‡πÄ‡∏Ç‡πâ‡∏≤ Pinecone ‡∏ó‡∏µ‡∏•‡∏∞‡∏Å‡πâ‡∏≠‡∏ô‡πÉ‡∏´‡∏ç‡πà
    print(f"‚òÅÔ∏è Uploading {len(vectors_to_upsert)} vectors to Pinecone...")
    for batch_vec in batch_iterate(vectors_to_upsert, BATCH_SIZE):
        index.upsert(vectors=batch_vec)

    # Clean up
    if os.path.exists(REPO_PATH):
        shutil.rmtree(REPO_PATH)
        
    print("‚úÖ Ingestion Complete!")
    return {"status": "success", "chunks": len(documents)}